import os
import glob
import pickle
from typing import List

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


def load_pdfs(pdf_paths: List[str]):
    """
    Load text documents from given PDF file paths.
    """
    documents = []
    for file_path in pdf_paths:
        try:
            loader = PyPDFLoader(file_path)
            documents.extend(loader.load())
            print(f"‚úÖ Loaded {file_path}")
        except Exception as e:
            print(f"‚ùå Error loading {file_path}: {e}")
    return documents


def split_documents(documents, chunk_size: int = 1000, chunk_overlap: int = 200):
    """
    Split documents into smaller chunks for embedding.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(documents)


def create_vector_db(docs, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    """
    Create a FAISS vector store from documents using HuggingFace embeddings.
    """
    print("üìê Generating embeddings...")
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    return FAISS.from_documents(docs, embeddings)


def save_vector_db(vectorstore, output_path="faiss_store_pdfs.pkl"):
    """
    Save the FAISS vector store to a pickle file.
    """
    print(f"üíæ Saving vector DB to {output_path}...")
    with open(output_path, "wb") as f:
        pickle.dump(vectorstore, f)
    print("‚úÖ Vector DB saved successfully!")


def load_vector_db(input_path="faiss_store_pdfs.pkl"):
    """
    Load the FAISS vector store from a pickle file.
    """
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Vector DB file {input_path} not found.")
    with open(input_path, "rb") as f:
        vectorstore = pickle.load(f)
    print("üìÇ Vector DB loaded successfully!")
    return vectorstore


def create_vector_db_from_folder(pdf_folder: str, output_path="faiss_store_pdfs.pkl"):
    """
    Full pipeline: load all PDFs in folder ‚Üí split ‚Üí embed ‚Üí save vector DB.
    """
    print(f"üìÇ Scanning folder {pdf_folder} for PDFs...")
    pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))

    if not pdf_files:
        raise ValueError(f"No PDF files found in folder: {pdf_folder}")

    print(f"üìÑ Found {len(pdf_files)} PDF(s). Loading documents...")
    pdf_docs = load_pdfs(pdf_files)

    if not pdf_docs:
        raise ValueError("No documents were loaded from the provided PDFs.")

    print("‚úÇÔ∏è Splitting documents...")
    docs = split_documents(pdf_docs)

    vectorstore = create_vector_db(docs)

    save_vector_db(vectorstore, output_path)
    return vectorstore


# Example usage
if __name__ == "__main__":
    # Replace with your local PDF folder path
    pdf_folder = "./pdf"  # <-- folder containing multiple PDFs
    vectorstore = create_vector_db_from_folder(pdf_folder, "faiss_store_pdfs.pkl")
